{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNAzBFedfgJRvKEPvOqRgcl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maaz-Ai-enthusiast/Gen_Ai_Data/blob/main/Module_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "!pip install nltk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vMEw4--z0Q8H",
        "outputId": "0ee44c4b-7b2e-4691-b539-d16a1be1a50f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stemming and tokanization code**"
      ],
      "metadata": {
        "id": "S7bAnjyK7CU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n"
      ],
      "metadata": {
        "id": "uwXhlOJF0fnw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9lmut5l0lTH",
        "outputId": "6a2b3157-a490-4d27-de14-f42df957d9e3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import the necessary modules\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Step 2: Download the 'punkt' tokenizer data\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Step 3: Initialize the PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Step 4: Define a function for stemming\n",
        "def perform_stemming(text):\n",
        "\n",
        "    # Step 5: Tokenize the input text into words\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Step 6: Print the tokenized words\n",
        "    print(\"Tokenized Words: \", words)\n",
        "\n",
        "    # Step 7: Create an empty list to store stemmed words\n",
        "    stemmed_words = []\n",
        "\n",
        "    # Step 8: Loop through each word in the tokenized list\n",
        "    for word in words:\n",
        "\n",
        "        # Step 9: Stem the word\n",
        "        stemmed_word = stemmer.stem(word)\n",
        "\n",
        "        # Step 10: Append the stemmed word to the list\n",
        "        stemmed_words.append(stemmed_word)\n",
        "\n",
        "    # Step 11: Print the stemmed words\n",
        "    print(\"Stemmed Words: \", stemmed_words)\n",
        "\n",
        "    # Step 12: Join the stemmed words back into a single string\n",
        "    stemmed_text = ' '.join(stemmed_words)\n",
        "\n",
        "    # Step 13: Return the final stemmed text\n",
        "    return stemmed_text\n",
        "\n",
        "# Step 14: Get input text from the user\n",
        "input_text = input(\"Enter the text to be stemmed: \")\n",
        "\n",
        "# Step 15: Call the perform_stemming function with the input text\n",
        "stemmed_text = perform_stemming(input_text)\n",
        "\n",
        "# Step 16: Print the original input text\n",
        "print(\"\\nOriginal Text: \", input_text)\n",
        "\n",
        "# Step 17: Print the final stemmed text\n",
        "print(\"Stemmed Text: \", stemmed_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "KDynGofE0nBL",
        "outputId": "2a404fe5-852a-49fe-da52-45186045337b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the text to be stemmed: whiteboard\n",
            "Tokenized Words:  ['whiteboard']\n",
            "Stemmed Words:  ['whiteboard']\n",
            "\n",
            "Original Text:  whiteboard\n",
            "Stemmed Text:  whiteboard\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lemmatization Code with tokanization**"
      ],
      "metadata": {
        "id": "GM60IjGv63vC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')  # Additional WordNet data\n",
        "nltk.download('punkt')     # For tokenization\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jis0b8zd68zM",
        "outputId": "82696c10-e990-4aab-cc60-bc83d36d9904"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Define a function to perform lemmatization\n",
        "def perform_lemmatization(text):\n",
        "    # Tokenize the input text into words\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Apply lemmatization to each word\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    # Join the lemmatized words back into a single string\n",
        "    lemmatized_text = ' '.join(lemmatized_words)\n",
        "\n",
        "    return lemmatized_text\n",
        "\n",
        "# Example usage:\n",
        "input_text = \"The children are playing in the gardens and they have played well.\"\n",
        "lemmatized_text = perform_lemmatization(input_text)\n",
        "\n",
        "# Print the results\n",
        "print(\"Original Text: \", input_text)\n",
        "print(\"Lemmatized Text: \", lemmatized_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ag0aC3HP7fOO",
        "outputId": "e4459fb0-8505-4985-a655-6a9052307c6e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:  The children are playing in the gardens and they have played well.\n",
            "Lemmatized Text:  The child are playing in the garden and they have played well .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**only lemmatization code**"
      ],
      "metadata": {
        "id": "YgaTZGxm8deL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import the necessary modules from NLTK\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Step 2: Download the WordNet data\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')  # Optional, for additional WordNet data\n",
        "\n",
        "# Step 3: Initialize the WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Step 4: Define a function to perform lemmatization on input text\n",
        "def lemmatize_text(text):\n",
        "    # Step 5: Split the input text into individual words\n",
        "    words = text.split()\n",
        "\n",
        "    # Step 6: Create an empty list to store lemmatized words\n",
        "    lemmatized_words = []\n",
        "\n",
        "    # Step 7: Loop through each word in the list of words\n",
        "    for word in words:\n",
        "        # Step 8: Lemmatize the word\n",
        "        lemmatized_word = lemmatizer.lemmatize(word)\n",
        "\n",
        "        # Step 9: Append the lemmatized word to the list\n",
        "        lemmatized_words.append(lemmatized_word)\n",
        "\n",
        "    # Step 10: Join the lemmatized words back into a single string with spaces\n",
        "    lemmatized_text = ' '.join(lemmatized_words)\n",
        "\n",
        "    # Step 11: Return the final lemmatized text\n",
        "    return lemmatized_text\n",
        "\n",
        "# Step 12: Get input text from the user\n",
        "input_text = input(\"Enter the text to be lemmatized: \")\n",
        "\n",
        "# Step 13: Call the lemmatize_text function with the input text\n",
        "lemmatized_output = lemmatize_text(input_text)\n",
        "\n",
        "# Step 14: Print the original input text\n",
        "print(\"\\nOriginal Text: \", input_text)\n",
        "\n",
        "# Step 15: Print the lemmatized text\n",
        "print(\"Lemmatized Text:\", lemmatized_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujEM5jlj_qzx",
        "outputId": "9ee77c4b-12e9-4779-fca8-1612d35791a7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the text to be lemmatized: the children are playing\n",
            "\n",
            "Original Text:  the children are playing\n",
            "Lemmatized Text: the child are playing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GREEDY SAMPLING**   --->drawback( start repetions when length of geenrated text is increases)"
      ],
      "metadata": {
        "id": "ITq-j582bIt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import the necessary modules\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Step 2: Install the transformers library (if not already installed)\n",
        "!pip install transformers\n",
        "\n",
        "# Step 3: Specify the model name\n",
        "model_name = \"gpt2\"\n",
        "\n",
        "# Step 4: Load the pre-trained GPT-2 model\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Step 5: Load the GPT-2 tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Step 6: Get input text from the user\n",
        "input_text = input(\"Enter the prompt text: \")\n",
        "\n",
        "# Step 7: Encode the input text into token IDs\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# Step 8: Set the model to evaluation mode to disable dropout\n",
        "model.eval()\n",
        "\n",
        "# Step 9: Use the model to generate output tokens with greedy sampling\n",
        "with torch.no_grad():  # Ensure no gradients are calculated, as this is inference\n",
        "    output_ids = model.generate(\n",
        "        input_ids,        # The tokenized input text\n",
        "        max_length=100,    # The maximum length of the generated sequence\n",
        "        do_sample=False   # Use greedy sampling (always pick the highest probability token)\n",
        "    )\n",
        "\n",
        "# Step 10: Decode the output token IDs back to a string\n",
        "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Step 11: Print the generated text\n",
        "print(\"\\nGenerated Text: \", generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08_tS0SGbOgz",
        "outputId": "3d80ed56-272d-4655-8e2e-376a11d92bbc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Enter the prompt text: i want to learn gen ai applications using llm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated Text:  i want to learn gen ai applications using llm.\n",
            "\n",
            "I am using llm to build ai applications.\n",
            "\n",
            "I am using llm to build ai applications.\n",
            "\n",
            "I am using llm to build ai applications.\n",
            "\n",
            "I am using llm to build ai applications.\n",
            "\n",
            "I am using llm to build ai applications.\n",
            "\n",
            "I am using llm to build ai applications.\n",
            "\n",
            "I am using llm to build a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BEEM SEARCH **"
      ],
      "metadata": {
        "id": "8d3-jVgsc5H_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Step 1: Load pre-trained model and tokenizer\n",
        "model_name = \"gpt2\"  # You can change this to a different GPT model if you like\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Step 2: Get input text from the user\n",
        "input_text = input(\"Enter the prompt text: \")\n",
        "\n",
        "# Step 3: Tokenize the input text\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# Step 4: Generate text using beam search\n",
        "beam_output = model.generate(\n",
        "    input_ids,\n",
        "    max_length=50,        # Maximum length of the generated sequence\n",
        "    num_beams=5,          # Number of beams to use in beam search\n",
        "    early_stopping=True   # Stop when an end-of-sequence token is generated\n",
        ")\n",
        "\n",
        "# Step 5: Decode the output tokens to text\n",
        "generated_text = tokenizer.decode(beam_output[0], skip_special_tokens=True)\n",
        "\n",
        "# Step 6: Print the generated text\n",
        "print(\"\\nGenerated Text: \", generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNhBIxz6c8q2",
        "outputId": "33a715b2-f1bf-4057-9080-fbbfe2aa42e0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the prompt text: hi i am a boy how can i earn money using computer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated Text:  hi i am a boy how can i earn money using computer i am a boy how can i earn money using computer i am a boy how can i earn money using computer i am a boy how can i earn money using computer i am a boy how\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TOP-P**"
      ],
      "metadata": {
        "id": "VFN0PlaCeaHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "model_name = \"gpt2\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Get input text from the user\n",
        "input_text = input(\"Enter the prompt text: \")\n",
        "\n",
        "# Tokenize the input text\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# Generate text using top-p (nucleus) sampling\n",
        "top_p_output = model.generate(\n",
        "    input_ids,\n",
        "    max_length=50,        # Maximum length of the generated sequence\n",
        "    do_sample=True,       # Enable sampling\n",
        "    top_p=0.9,            # Set the cumulative probability threshold (p)\n",
        "    top_k=0,              # Set top_k=0 to deactivate top-k sampling\n",
        "    early_stopping=True   # Stop when an end-of-sequence token is generated\n",
        ")\n",
        "\n",
        "# Decode the output tokens to text\n",
        "generated_text = tokenizer.decode(top_p_output[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the generated text\n",
        "print(\"\\nGenerated Text: \", generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJKznmC4eccs",
        "outputId": "49b9d4ac-920b-498e-8276-a28a7c43d97a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the prompt text: i am learning gen ai course\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:588: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
            "  warnings.warn(\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated Text:  i am learning gen ai course wi n tnyne itsier si nu col li adjaine lch.may be.xUne.wera.to'i. Ae'y liChaux ti /k,t;\n"
          ]
        }
      ]
    }
  ]
}